HOMEcity = `HOME City`,
HOMEcounty = `HOME County`,
HOMEstate = `HOME State`,
funding_sources = `Funder`,
expiration_date = `Project Expiration Date`,
large_household = `Large Household (4+ pp)`,
site_type = `Site Type`,
senior = `Elderly`,
disabled = `Persons with Disabilities`,
reported_address = `Address`,
county = `County`,
farmworker = `Farmworker`,
homeless = `Homeless`,
transitional = `Transitional`,
data_source = `DataSource`,
veterans = `Veterans`,
zip = `Zip`)
#select only necessary columns and arrange columns
WSHFC_cleaned <- select_and_arrange_columns_function(WSHFC_cleaned)
#set DataSource field
WSHFC_cleaned$data_source = "WSHFC"
WSHFC_cleaned$reported_address[WSHFC_cleaned$reported_address == '1724 E. 44th'] <- '1724 E 44th Street'
WSHFC_cleaned$reported_address[WSHFC_cleaned$reported_address == '9225 Bayshore Drive NW'] <- '9225 Bay Shore Dr NW'
WSHFC_cleaned$reported_address[WSHFC_cleaned$reported_address == '9239 Bayshore Dr NW'] <- '9239 Bay Shore Dr NW'
# Clean address field for matching
# WSHFC_cleaned$full_address <- str_c(WSHFC_cleaned$reported_address,', ',WSHFC_cleaned$city,', WA, ',WSHFC_cleaned$zip)
# WSHFC_cleaned_test <- add_cleaned_addresses(WSHFC_cleaned)
#
# str(WSHFC_cleaned)
## 5) save file --------------------------------------------------------------------
#save cleaned file
write_csv(WSHFC_cleaned, paste0(WSHFC_path, WSHFC_clean_file))
## assumptions -------------------------
library(tidyverse)
library(tidyr)
library(readxl)
library(data.table)
library(magrittr)
library(stringr)
library(dplyr)
library(odbc)
library(DBI)
remotes::install_github("slu-openGIS/postmastr")
elmer_connection <- dbConnect(odbc::odbc(),
driver = "SQL Server",
server = "AWS-PROD-SQL\\Sockeye",
database = "Elmer",
trusted_connection = "yes")
WSHFC_path <- "J:/Projects/IncomeRestrictedHsgDB/2022 vintage/Data/WSHFC/WSHFC_2022_cleaned.csv"
#export_4review_path <- "C:/Users/eclute/OneDrive - Puget Sound Regional Council/Documents/GitHub/irhd/Export4review.csv"
#HASCO_updates_path <- "J:/Projects/IncomeRestrictedHsgDB/2022 vintage/Review Files - Received/PSRC_2022_IRHD_Snohomish_minor updates.csv"
#THA_updates_path <- "J:/Projects/IncomeRestrictedHsgDB/2022 vintage/Review Files - Received/PSRC_2022_IRHD_Pierce_THA_minor updates.csv"
#KC_path <- "J:/Projects/IncomeRestrictedHsgDB/2022 vintage/Review Files - Received/King County Income-restricted Housing Database 2022.csv"
address_script <- "C:/Users/eclute/OneDrive - Puget Sound Regional Council/Documents/GitHub/irhd/address_match.R"
source(address_script)
`%not_in%` <- Negate(`%in%`)
vintage_year <- 2022
last_vintage <- vintage_year - 1
sql_import <- paste('irhd.properties')
sql_export <- paste('exec irhd.merge_irhd_properties', vintage_year)
# functions ---
# BY COUNTY
summary_county <- function(df){
new_IRHD_county <- df %>%
group_by(county) %>%
summarize("unit count" = sum(na.omit(total_restricted_units)))
# add total column
new_IRHD_county <- new_IRHD_county %>%
bind_rows(summarise(., across(where(is.numeric), sum),
across(where(is.character), ~'Total')))
#transpose
new_IRHD_county <- transpose(new_IRHD_county, keep.names = 'county')
#fix column names
colnames(new_IRHD_county) <- new_IRHD_county[1,]
new_IRHD_county <- new_IRHD_county[-1, ]
new_IRHD_county %<>% rename(!!paste(vintage_year, "new units") := "county")
}
# BY UNIT SIZE
summary_county_bedrooms <- function(df){
IRHD_county_bedrooms <- df %>%
group_by(county) %>%
summarize(`studio and one bedrooms` = sum(na.omit(bedroom_0 + bedroom_1)),`two and three bedrooms` = sum(na.omit(bedroom_2 + bedroom_3)),`four bedrooms and more` = sum(na.omit(bedroom_4 + bedroom_5)),`Unknown Size` = sum(na.omit(bedroom_Unknown)))
# add total column
IRHD_county_bedrooms <- IRHD_county_bedrooms %>%
bind_rows(summarise(., across(where(is.numeric), sum),
across(where(is.character), ~'Total')))
# add total row
IRHD_county_bedrooms %<>% mutate(total=rowSums(select_if(., is.numeric)))
#transpose
IRHD_county_bedrooms <- transpose(IRHD_county_bedrooms, keep.names = 'county')
#fix column names
colnames(IRHD_county_bedrooms) <- IRHD_county_bedrooms[1,]
IRHD_county_bedrooms <- IRHD_county_bedrooms[-1, ]
IRHD_county_bedrooms %<>% rename("unit_size" = "county")
}
# BY AMI LIMIT
summary_county_ami <- function(df){
IRHD_county_ami <- df %>%
group_by(county) %>%
summarize(`less than 30` = sum(na.omit(ami_20 + ami_25 + ami_30)),`31 to 50` = sum(na.omit(ami_35 + ami_40 + ami_45 +ami_50)),`51 to 80` = sum(na.omit(ami_60 + ami_65 + ami_70 + ami_75 + ami_80)),`81 to 100` = sum(na.omit(ami_85 + ami_90 + ami_100)),`100 plus` = sum(na.omit(ami_120)),`unknown AMI` = sum(na.omit(ami_unknown)))
# add total column
IRHD_county_ami <- IRHD_county_ami %>%
bind_rows(summarise(., across(where(is.numeric), sum),
across(where(is.character), ~'Total')))
# add total row
IRHD_county_ami %<>% mutate(total=rowSums(select_if(., is.numeric)))
#transpose
IRHD_county_ami <- transpose(IRHD_county_ami, keep.names = 'county')
#fix column names
colnames(IRHD_county_ami) <- IRHD_county_ami[1,]
IRHD_county_ami <- IRHD_county_ami[-1, ]
IRHD_county_ami %<>% rename("ami_limits" = "county")
}
## 1) load data -------------------------
# load last vintage from Elmer
IRHD_raw <- dbReadTable(elmer_connection, SQL(sql_import))
# borrow datatype characterization from IRHD to apply to identical columns in WSHFC data
# irhd_colClasses = sapply(IRHD_raw, class)
# names(irhd_colClasses) <- colnames(IRHD_raw)
# WSHFC_cols = colnames(read.csv(WSHFC_path, nrows=1))
# wshfc_colClasses <- irhd_colClasses %>% .[names(.) %in% WSHFC_cols]
# load cleaned WSHFC data that has portfolios as of end of 2022; apply datatypes to match
# WSHFC_raw <- fread(WSHFC_path, colClasses=wshfc_colClasses)
WSHFC_raw <- fread(WSHFC_path)
# load cleaned KC data that has portfolios as of end of 2022
# KC_raw <- fread(KC_path)
# load cleaned HASCO & THA data - only keep fields where we have new data (in the "Corrected" column)
# HASCO_raw <- fread(HASCO_updates_path)
# HASCO <- HASCO_raw %>%
#   drop_na(Corrected)
#
# THA_raw <- fread(THA_updates_path)
# THA <- THA_raw %>%
#   drop_na(Corrected)
## 2) clean up data -------------------------
# IRHD ---
IRHD_raw <- IRHD_raw %>% filter(data_year == last_vintage)
IRHD <- IRHD_raw %>% filter(!(county == "King"))  # King county handled separately
# Remove unneeded fields
IRHD %<>% select(-c(created_at,updated_at,sro,shape))
# King County finalized data ---
# KC <- KC_raw
# KC$county <- "King"
# KC %<>% filter(KC$in_service_date <= vintage_year | is.na(KC$in_service_date))
# Remove fields we don't need
##(Policy field is blank, data currently stored in "FundingSource" - This may change!! Watch next year)
# KC %<>% select(-c(unique_linking_ID,HITS_survey,GeoCode_Street,GeoCode_City,ProjectType,Policy))
# Rename fields to match IRHD
# KC <- KC %>%
#   rename("data_source" = "DataSourceName",
#          "bed_count" = "GroupHomeOrBed",
#          "zip" = "GeoCode_Zip",
#          "full_address" = "address_standardized",
#          "expiration_date" = "ExpirationYear",
#          "property_owner" = "ProjectSponsor",
#          "manager" = "ContactName",
#          "site_type" = "PopulationServed",
#          "funding_sources" = "Funder",
#          "HOME" = "HOMEUnits",
#          "policy" = "FundingSource")
#
# KC$cleaned.address <- str_c(KC$full_address,', ',KC$city,', WA, ',KC$zip)
# Identify and remove duplicated working_id value
# dups <- filter(KC, working_id == "SH_5215")
# KC[1222,1]<-"SH_7234"
# rm(dups)
## 3) clean up some variables in WSHFC before joining -------------------------
newWSHFC <- anti_join(WSHFC_raw, IRHD, by = "property_id")
str(WSHFC_raw)
class(WSHFC_raw$project_id) = "numeric"
newWSHFC <- anti_join(WSHFC_raw, IRHD, by = "property_id")
class(WSHFC_raw$property_id) = "numeric"
newWSHFC <- anti_join(WSHFC_raw, IRHD, by = "property_id")
install.packages("tidycensus")
install.packages("psrccensus")
install.packages(c("dbplyr", "evaluate", "htmltools", "httpuv", "httr2", "knitr", "leaflet", "lifecycle", "maps", "markdown", "plotly", "RcppEigen", "rlang", "rprojroot", "RSQLite", "stringi", "utf8", "waldo", "withr", "wk", "xfun", "XML"))
true
library(psrccensus)
library(openxlsx)
library(tidycensus)
library(tidyverse)
# years of interest (applies to all functions below)
years <- c(2019,2021)
create_mmh_owner_summary_table <- function(year) {
#---------------------Grab data from Census API------------------------
mmh_raw<-get_acs_recs(geography = 'county',
table.names = c('B25032'),
years = years,
counties = c("King", "Kitsap", "Pierce", "Snohomish"),
acs.type = 'acs1')
#---------------------Create custom groupings------------------------
# The next step is to create the appropriate grouping variable (using pipes for simplicity)
mmh_coded <- mmh_raw %>%
mutate(building_size=factor(case_when(grepl("_003$", variable) ~ "Single Family",
grepl("_004$|_005$|_006$", variable) ~ "2-4 units",
grepl("_007$|_008$", variable) ~ "5-19 units",
grepl("_009$|_010$", variable) ~ "20+ units",
grepl("_011$|_012$", variable) ~ "Mobile Home/Other",
TRUE ~ NA_character_),
levels=c("Single Family","2-4 units","5-19 units", "20+ units", "Mobile Home/Other"))) %>%
mutate(building_size_2=factor(case_when(grepl("_003$", variable) ~ "Single Family",
grepl("_004$|_005$|_006$|_007$|_008$", variable) ~ "2-19 units",
grepl("_009$|_010$", variable) ~ "20+ units",
grepl("_011$|_012$", variable) ~ "Mobile Home/Other",
TRUE ~ NA_character_),
levels=c("Single Family","2-19 units", "20+ units", "Mobile Home/Other")
))
#--------------------Aggregate data, incorporate 2-19 Unit group------------------------
# In this step, you create an aggregate, using the grouping you created in the last call.
mmh_agg_owner <- summarize(mmh_raw, estimate=sum(estimate, na.rm=TRUE), moe=moe_sum(moe=moe, estimate=estimate, na.rm=TRUE))
# In this step, you create an aggregate, using the first grouping you created in the last call.
mmh_agg_owner_01 <- mmh_coded %>%
group_by(across(c(name, year, building_size))) %>%
summarize(estimate=sum(estimate, na.rm=TRUE), moe=moe_sum(moe=moe, estimate=estimate, na.rm=TRUE))
# In this step, you create an aggregate, using the second grouping you created.
mmh_agg_owner_02 <- mmh_coded %>%
group_by(across(c(name, year, building_size_2))) %>%
summarize(estimate=sum(estimate, na.rm=TRUE), moe=moe_sum(moe=moe, estimate=estimate, na.rm=TRUE)) %>%
filter(building_size_2 == '2-19 units') %>%
rename(building_size = building_size_2)
df <- mmh_agg_owner_01 %>%
bind_rows(mmh_agg_owner_02)
}
all_owner_tables <- map(years, ~create_mmh_owner_summary_table(.x)) %>%
reduce(bind_rows)
mmh_owner <- all_owner_tables %>%
mutate(building_size = factor(building_size,
levels = c('Single Family', '2-4 units', '5-19 units', '2-19 units', '20+ units', 'Mobile Home/Other'))) %>%
arrange(year, name, building_size) %>%
filter(building_size != is.na(building_size))
rm(list = setdiff(ls(), c("mmh_owner", "create_mmh_owner_summary_table", "years")))
View(mmh_owner)
# years of interest (applies to all functions below)
years <- c(2019,2022)
all_owner_tables <- map(years, ~create_mmh_owner_summary_table(.x)) %>%
reduce(bind_rows)
mmh_owner <- all_owner_tables %>%
mutate(building_size = factor(building_size,
levels = c('Single Family', '2-4 units', '5-19 units', '2-19 units', '20+ units', 'Mobile Home/Other'))) %>%
arrange(year, name, building_size) %>%
filter(building_size != is.na(building_size))
rm(list = setdiff(ls(), c("mmh_owner", "create_mmh_owner_summary_table", "years")))
View(mmh_owner)
create_mmh_renter_summary_table <- function(year) {
#---------------------Grab data from Census API------------------------
mmh_raw<-get_acs_recs(geography = 'county',
table.names = c('B25032'),
years = year,
counties = c("King", "Kitsap", "Pierce", "Snohomish"),
acs.type = 'acs1')
#---------------------Create custom groupings------------------------
# The next step is to create the appropriate grouping variable (using pipes for simplicity)
mmh_coded <- mmh_raw %>%
mutate(building_size=factor(case_when(grepl("_014$", variable) ~ "Single Family",
grepl("_015$|_016$|_017$", variable) ~ "2-4 units",
grepl("_018$|_019$", variable) ~ "5-19 units",
grepl("_020$|_021$", variable) ~ "20+ units",
grepl("_022$|_023$", variable) ~ "Mobile Home/Other",
TRUE ~ NA_character_),
levels=c("Single Family","2-4 units","5-19 units", "20+ units", "Mobile Home/Other"))) %>%
mutate(building_size_2=factor(case_when(grepl("_014$", variable) ~ "Single Family",
grepl("_015$|_016$|_017$|_018$|_019$", variable) ~ "2-19 units",
grepl("_020$|_021$", variable) ~ "20+ units",
grepl("_022$|_023$", variable) ~ "Mobile Home/Other",
TRUE ~ NA_character_),
levels=c("Single Family","2-19 units", "20+ units", "Mobile Home/Other")
))
#--------------------Aggregate data, incorporate 2-19 Unit group------------------------
# In this step, you create an aggregate, using the grouping you created in the last call.
mmh_agg_renter <- summarize(mmh_raw, estimate=sum(estimate, na.rm=TRUE), moe=moe_sum(moe=moe, estimate=estimate, na.rm=TRUE))
# In this step, you create an aggregate, using the first grouping you created in the last call.
mmh_agg_renter_01 <- mmh_coded %>%
group_by(across(c(name, year, building_size))) %>%
summarize(estimate=sum(estimate, na.rm=TRUE), moe=moe_sum(moe=moe, estimate=estimate, na.rm=TRUE))
# In this step, you create an aggregate, using the second grouping you created.
mmh_agg_renter_02 <- mmh_coded %>%
group_by(across(c(name, year, building_size_2))) %>%
summarize(estimate=sum(estimate, na.rm=TRUE), moe=moe_sum(moe=moe, estimate=estimate, na.rm=TRUE)) %>%
filter(building_size_2 == '2-19 units') %>%
rename(building_size = building_size_2)
df <- mmh_agg_renter_01 %>%
bind_rows(mmh_agg_renter_02)
}
all_renter_tables <- map(years, ~create_mmh_renter_summary_table(.x)) %>%
reduce(bind_rows)
mmh_renter <- all_renter_tables %>%
mutate(building_size = factor(building_size,
levels = c('Single Family', '2-4 units', '5-19 units', '2-19 units', '20+ units', 'Mobile Home/Other'))) %>%
arrange(year, name, building_size) %>%
filter(building_size != is.na(building_size))
rm(list = setdiff(ls(), c("mmh_renter", "create_mmh_renter_summary_table", "mmh_owner", "create_mmh_owner_summary_table", "years")))
#------------------------Summarize existing housing stock------------------------
calc_share_growth <- function(table) {
# calculate totals in new dataframe
totals <- table %>%
filter(building_size != '2-19 units') %>%
group_by(name, year) %>%
summarise(total = sum(estimate))
# join to main table
table <- table %>%
left_join(totals, by = c('name', 'year'))
# calculate % of units by building size and % growth between years
table <- table %>%
mutate(share = estimate/total) %>%
arrange(name, building_size) %>%
group_by(name, building_size) %>%
mutate(growth = estimate-lag(estimate)) %>%
mutate(growth_share = growth/lag(total)) %>%
arrange(factor(name, levels = c('King County', 'Kitsap County', 'Pierce County', 'Snohomish County', 'Region')))
}
pivot_to_wide <- function(table) {
# wide format
growth_cols_head <- c(paste0('growth_', years[1:(length(years)-1)]), paste0('growth_share_', years[1:(length(years)-1)]))
growth_cols_tail <- rep(str_sub(years[-1], start = 3, end = 4), 2)
new_colnames <- map2(growth_cols_head, growth_cols_tail, ~paste0(.x, '-', .y)) %>% unlist()
old_colnames <- c(paste0('growth_', years[-1]), paste0('growth_share_', years[-1]))
names(old_colnames) <- new_colnames
# pivot to wide format
df <- table %>%
pivot_wider(id_cols = c('name', 'building_size'),
names_from = year,
values_from = c('estimate', 'moe', 'total', 'share', 'growth', 'growth_share'))
# rename growth columns
df <- df %>%
rename(all_of(old_colnames)) %>%
select(-ends_with(paste0('growth_', years[1])), -ends_with(paste0('growth_share_', years[1])))
}
# long format
df_mmh_owner <- calc_share_growth(mmh_owner)
df_mmh_renter <- calc_share_growth(mmh_renter)
# wide format (for excel)
df_mmh_owner_wide <- pivot_to_wide(df_mmh_owner)
df_mmh_renter_wide <- pivot_to_wide(df_mmh_renter)
share_cols_owner <- str_which(colnames(df_mmh_owner_wide), 'share')
share_cols_renter <- str_which(colnames(df_mmh_renter_wide), 'share')
View(df_mmh_owner_wide)
View(df_mmh_renter_wide)
library(tidyverse)
library(tidyr)
library(readxl)
library(data.table)
library(magrittr)
library(stringr)
library(dplyr)
library(odbc)
library(DBI)
remotes::install_github("slu-openGIS/postmastr")
elmer_connection <- dbConnect(odbc::odbc(),
driver = "SQL Server",
server = "AWS-PROD-SQL\\Sockeye",
database = "Elmer",
trusted_connection = "yes")
export_4review_housingauthorities <- "./Export4review-housingauthorities.csv"
export_4review_wshfc <- "./Export4review-wshfc.csv"
#HASCO_updates_path <- ""
#THA_updates_path <- ""
address_script <- "./address_match.R"
summary_func <- "./summary_func_irhd.R"
wshfc_clean_script <- "./clean_2022_WSHFC_data.R"
kc_clean_script <- "./clean_2022_KC_data.R"
source(address_script)
# load all IRHD records from Elmer
IRHD_raw <- dbReadTable(elmer_connection, SQL(sql_import))
source(address_script)
setwd("C:/Users/eclute/GitHub/irhd")
setwd("C:/Users/eclute/GitHub/irhd")
library(tidyverse)
library(tidyr)
library(readxl)
library(data.table)
library(magrittr)
library(stringr)
library(dplyr)
library(odbc)
library(DBI)
remotes::install_github("slu-openGIS/postmastr")
elmer_connection <- dbConnect(odbc::odbc(),
driver = "SQL Server",
server = "AWS-PROD-SQL\\Sockeye",
database = "Elmer",
trusted_connection = "yes")
export_4review_housingauthorities <- "./Export4review-housingauthorities.csv"
export_4review_wshfc <- "./Export4review-wshfc.csv"
#HASCO_updates_path <- ""
#THA_updates_path <- ""
address_script <- "./address_match.R"
summary_func <- "./summary_func_irhd.R"
wshfc_clean_script <- "./clean_2022_WSHFC_data.R"
kc_clean_script <- "./clean_2022_KC_data.R"
source(address_script)
source(summary_func)
`%not_in%` <- Negate(`%in%`)
vintage_year <- 2022
last_vintage <- vintage_year - 1
sql_import <- paste('irhd.properties')
sql_export <- paste('exec irhd.merge_irhd_properties', vintage_year)
# load all IRHD records from Elmer
IRHD_raw <- dbReadTable(elmer_connection, SQL(sql_import))
View(IRHD_raw)
# load cleaned WSHFC data
source(wshfc_clean_script)
IRHD_raw <- IRHD_raw %>% filter(data_year == last_vintage)
IRHD <- IRHD_raw %>% filter(!(county == "King")) # King county handled separately
IRHD %<>% select(-c(created_at,updated_at,sro,shape)) # Remove unneeded fields
irhd_colClasses = sapply(IRHD_raw, class)
names(irhd_colClasses) <- colnames(IRHD_raw)
WSHFC_cols = colnames(WSHFC_cleaned)
wshfc_colClasses <- irhd_colClasses %>% .[names(.) %in% WSHFC_cols]
WSHFC_cleaned1[] <- mapply(FUN = as,WSHFC_cleaned1,sapply(IRHD,class),SIMPLIFY = FALSE)
WSHFC_cleaned1 <- WSHFC_cleaned
WSHFC_cleaned1[] <- mapply(FUN = as,WSHFC_cleaned1,sapply(IRHD,class),SIMPLIFY = FALSE)
matchColClasses<- function(df1, df2){
# Purpose:  protect joins from column type mismatches - a problem with multi-column empty df
# Input:    df1 - master for class assignments, df2 - for col reclass and return.
# Output:   df2 with shared columns classed to match df1
# Usage:    df2 <- matchColClasses(df1, df2)
sharedColNames <- names(df1)[names(df1) %in% names(df2)]
sharedColTypes <- sapply(df1[,sharedColNames], class)
for (n in sharedColNames) {
class(df2[, n]) <- sharedColTypes[n]
}
return(df2)
}
matchColClasses(IRHD,WSHFC_cleaned1)
matchColClasses(unlists(IRHD),unlist(WSHFC_cleaned1))
matchColClasses(unlist(IRHD),unlist(WSHFC_cleaned1))
matchColClasses(IRHD,WSHFC_cleaned1)
IRHD1 <- IRHD
unlist(WSHFC_cleaned1)
WSHFC_cleaned1 <- unlist(WSHFC_cleaned)
IRHD1 <- unlist(IRHD)
matchColClasses(IRHD1,WSHFC_cleaned1)
setwd("C:/Users/eclute/GitHub/irhd")
library(tidyverse)
library(tidyr)
library(readxl)
library(data.table)
library(magrittr)
library(stringr)
library(dplyr)
library(odbc)
library(DBI)
remotes::install_github("slu-openGIS/postmastr")
elmer_connection <- dbConnect(odbc::odbc(),
driver = "SQL Server",
server = "AWS-PROD-SQL\\Sockeye",
database = "Elmer",
trusted_connection = "yes")
export_4review_housingauthorities <- "./Export4review-housingauthorities.csv"
export_4review_wshfc <- "./Export4review-wshfc.csv"
#HASCO_updates_path <- ""
#THA_updates_path <- ""
address_script <- "./address_match.R"
summary_func <- "./summary_func_irhd.R"
wshfc_clean_script <- "./clean_2022_WSHFC_data.R"
kc_clean_script <- "./clean_2022_KC_data.R"
source(address_script)
source(summary_func)
`%not_in%` <- Negate(`%in%`)
vintage_year <- 2022
last_vintage <- vintage_year - 1
sql_import <- paste('irhd.properties')
sql_export <- paste('exec irhd.merge_irhd_properties', vintage_year)
## 1) load data -------------------------
# load all IRHD records from Elmer
IRHD_raw <- dbReadTable(elmer_connection, SQL(sql_import))
# load cleaned WSHFC data
source(wshfc_clean_script)
# load cleaned data from data partners
#source(kc_clean_script)
# Only keep fields where we have new data (in the "Corrected" column)
# HASCO_raw <- fread(HASCO_updates_path)
# HASCO <- HASCO_raw %>%
#   drop_na(Corrected)
#
# THA_raw <- fread(THA_updates_path)
# THA <- THA_raw %>%
#   drop_na(Corrected)
## 2) Final tweaks -------------------------
IRHD_raw <- IRHD_raw %>% filter(data_year == last_vintage)
IRHD <- IRHD_raw %>% filter(!(county == "King")) # King county handled separately
IRHD %<>% select(-c(created_at,updated_at,sro,shape)) # Remove unneeded fields
# borrow field types from IRHD for identical fields in WSHFC
irhd_colClasses = sapply(IRHD_raw, class)
names(irhd_colClasses) <- colnames(IRHD_raw)
WSHFC_cols = colnames(WSHFC_cleaned)
wshfc_colClasses <- irhd_colClasses %>% .[names(.) %in% WSHFC_cols]
change_class2 <- function(predict_set, train_set) {
for (column in colnames(predict_set)) {
class(predict_set[, column]) <- class(train_set[, column])
}
}
WSHFC_cleaned1 <- WSHFC_cleaned
IRHD1 <- IRHD
change_class2(WSHFC_cleaned1, IRHD1)
# apply field types for identical fields
WSHFC_cleaned1 <- fread(WSHFC_cleaned, colClasses=wshfc_colClasses) #Isn't working right yet, tbd!
change_class2(WSHFC_cleaned1, IRHD1)
WSHFC_cleaned1 <- unlist(WSHFC_cleaned)
IRHD1 <- unlist(IRHD)
change_class2(WSHFC_cleaned1, IRHD1)
list(WSHFC_cleaned1)
WSHFC_cleaned1 <- list(WSHFC_cleaned1)
View(WSHFC_cleaned1)
WSHFC_cleaned1 <- frame(WSHFC_cleaned1)
WSHFC_cleaned1 <- unlist(WSHFC_cleaned)
IRHD1 <- unlist(IRHD)
change_class2(WSHFC_cleaned1, irhd_colClasses)
WSHFC_cleaned1 <- frame(WSHFC_cleaned1)
WSHFC_cleaned1 <- list(WSHFC_cleaned1)
View(WSHFC_cleaned1)
View(WSHFC_cleaned1)
